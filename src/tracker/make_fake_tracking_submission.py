import argparse
import glob
import os
import uuid
from itertools import zip_longest

import tensorflow as tf
from tqdm import tqdm
from waymo_open_dataset import dataset_pb2
from waymo_open_dataset import dataset_pb2 as open_dataset
from waymo_open_dataset import label_pb2
from waymo_open_dataset.protos import metrics_pb2


def get_objects(tf_root):
    objects = metrics_pb2.Objects()
    frame = open_dataset.Frame()
    files = glob.glob(os.path.join(tf_root, "*.tfrecord"))
    for label_type in range(3):
        for file_name in tqdm(files[:]):
            dataset = tf.data.TFRecordDataset(file_name, compression_type='')
            frames = dict()
            detections = dict()
            for idx, data in enumerate(dataset):
                frame.ParseFromString(bytearray(data.numpy()))
                context_name = frame.context.name
                timestamp_micros = frame.timestamp_micros
                for camera_image, camera_label in zip_longest(frame.images, frame.camera_labels):
                    camera_name = open_dataset.CameraName.Name.Name(camera_image.name)
                    if camera_name not in frames.keys():
                        frames[camera_name] = list()
                    if camera_name not in detections.keys():
                        detections[camera_name] = list()
                    # frames[camera_name].append(tf.image.decode_jpeg(camera_image.image).numpy())
                    d = list()
                    if camera_label is not None:
                        for label in camera_label.labels:
                            if label_type == label.type-1:
                                x = label.box.center_x - 0.5 * label.box.length
                                y = label.box.center_y - 0.5 * label.box.width
                                width = label.box.length
                                height = label.box.width
                                d.append({'bbox': (x, y, x + width, y + height), 'score': 1.0, 'class': label.type - 1})
                    detections[camera_name].append(d)

            tracks = dict()
            for key in detections.keys():
                tracks[key] = track_iou(detections[key], sigma_l=0.0, sigma_h=0.5, sigma_iou=0.25, t_min=2)

            active_tracks, start_track, len_track = dict(), dict(), dict()
            for key in tracks.keys():
                active_tracks[key] = []
                for track in tracks[key]:
                    track['len_track'] = len(track['bboxes'])
                    track['object_id'] = str(uuid.uuid1())

            for idx, data in enumerate(dataset, start=1):
                frame.ParseFromString(bytearray(data.numpy()))
                context_name = frame.context.name
                timestamp_micros = frame.timestamp_micros
                for camera_image, camera_label in zip_longest(frame.images, frame.camera_labels):
                    camera_name = open_dataset.CameraName.Name.Name(camera_image.name)
                    active_tracks[camera_name] = list(filter(lambda x: x['start_frame'] + x['len_track'] > idx, active_tracks[camera_name]))
                    active_tracks[camera_name] += list(filter(lambda x: x['start_frame'] == idx, tracks[camera_name]))
                    for track in active_tracks[camera_name]:
                        pos = idx - track['start_frame']

                        o = metrics_pb2.Object()
                        o.context_name = context_name
                        o.frame_timestamp_micros = timestamp_micros
                        o.camera_name = getattr(dataset_pb2.CameraName, camera_name)

                        box = label_pb2.Label.Box()
                        b = track['bboxes'][pos]
                        box.center_x = b[0] + 0.5 * b[2]
                        box.center_y = b[1] + 0.5 * b[3]
                        box.length = b[2]
                        box.width = b[3]
                        o.object.box.CopyFrom(box)
                        # This must be within [0.0, 1.0]. It is better to filter those boxes with
                        # small scores to speed up metrics computation.
                        o.score = 1.0
                        # For tracking, this must be set and it must be unique for each tracked
                        # sequence.
                        o.object.id = track['object_id']
                        # Use correct type.
                        o.object.type = label_type + 1
                        objects.objects.append(o)
    return objects


def track_iou(detections, sigma_l, sigma_h, sigma_iou, t_min):
    """
    Simple IOU based tracker.
    See "High-Speed Tracking-by-Detection Without Using Image Information by E. Bochinski, V. Eiselein, T. Sikora" for
    more information.

    Args:
         detections (list): list of detections per frame, usually generated by util.load_mot
         sigma_l (float): low detection threshold.
         sigma_h (float): high detection threshold.
         sigma_iou (float): IOU threshold.
         t_min (float): minimum track length in frames.

    Returns:
        list: list of tracks.
    """

    tracks_active = []
    tracks_finished = []

    for frame_num, detections_frame in enumerate(detections, start=1):
        # apply low threshold to detections
        dets = [det for det in detections_frame if det['score'] >= sigma_l]

        updated_tracks = []
        for track in tracks_active:
            if len(dets) > 0:
                # get det with highest iou
                best_match = max(dets, key=lambda x: iou(track['bboxes'][-1], x['bbox']))
                if iou(track['bboxes'][-1], best_match['bbox']) >= sigma_iou:
                    track['bboxes'].append(best_match['bbox'])
                    track['max_score'] = max(track['max_score'], best_match['score'])

                    updated_tracks.append(track)

                    # remove from best matching detection from detections
                    del dets[dets.index(best_match)]

            # if track was not updated
            if len(updated_tracks) == 0 or track is not updated_tracks[-1]:
                # finish track when the conditions are met
                if track['max_score'] >= sigma_h and len(track['bboxes']) >= t_min:
                    tracks_finished.append(track)

        # create new tracks
        new_tracks = [{'bboxes': [det['bbox']], 'max_score': det['score'], 'start_frame': frame_num} for det in dets]
        tracks_active = updated_tracks + new_tracks

    # finish all remaining active tracks
    tracks_finished += [track for track in tracks_active
                        if track['max_score'] >= sigma_h and len(track['bboxes']) >= t_min]

    return tracks_finished


def iou(bbox1, bbox2):
    """
    Calculates the intersection-over-union of two bounding boxes.

    Args:
        bbox1 (numpy.array, list of floats): bounding box in format x1,y1,x2,y2.
        bbox2 (numpy.array, list of floats): bounding box in format x1,y1,x2,y2.

    Returns:
        int: intersection-over-onion of bbox1, bbox2
    """

    bbox1 = [float(x) for x in bbox1]
    bbox2 = [float(x) for x in bbox2]

    (x0_1, y0_1, x1_1, y1_1) = bbox1
    (x0_2, y0_2, x1_2, y1_2) = bbox2

    # get the overlap rectangle
    overlap_x0 = max(x0_1, x0_2)
    overlap_y0 = max(y0_1, y0_2)
    overlap_x1 = min(x1_1, x1_2)
    overlap_y1 = min(y1_1, y1_2)

    # check if there is an overlap
    if overlap_x1 - overlap_x0 <= 0 or overlap_y1 - overlap_y0 <= 0:
        return 0

    # if yes, calculate the ratio of the overlap to each ROI size and the unified size
    size_1 = (x1_1 - x0_1) * (y1_1 - y0_1)
    size_2 = (x1_2 - x0_2) * (y1_2 - y0_2)
    size_intersection = (overlap_x1 - overlap_x0) * (overlap_y1 - overlap_y0)
    size_union = size_1 + size_2 - size_intersection

    return size_intersection / size_union


parser = argparse.ArgumentParser(description='FakeValTrackingSubmission')
parser.add_argument('--tf-root', type=str)
parser.add_argument('--output', type=str)
if __name__ == '__main__':
    args = parser.parse_args()
    objects = get_objects(args.tf_root)
    with open(args.output, 'wb') as f:
        f.write(objects.SerializeToString())
